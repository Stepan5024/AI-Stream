import matplotlib.pyplot as plt

# Данные
steps = [49, 99, 149, 199, 249, 299, 349, 399, 449, 499, 549, 599, 649, 699, 749, 799, 849, 899, 949, 999, 1049, 1099, 1149, 1199, 1249, 1299, 1349, 1399, 1449, 1499, 1549, 1599, 1649, 1699, 1749, 1799, 1849, 1899, 1949, 1999, 2049, 2099, 2149, 2199, 2249, 2299, 2349, 2399, 2449, 2499, 2549, 2599, 2649, 2699, 2749, 2799, 2849, 2899, 2949, 2999, 3049, 3099, 3149, 3199, 3249, 3299, 3349, 3399, 3449, 3499, 3549, 3599, 3649, 3699, 3749, 3799, 3849, 3899, 3949, 3999, 4049, 4099, 4149, 4199, 4249, 4299, 4349, 4399, 4449, 4499, 4549, 4599, 4649, 4699, 4749, 4799, 4849, 4899, 4949, 4999, 5049, 5099, 5149, 5199, 5249, 5299, 5349, 5399, 5449, 5499, 5549, 5599, 5649, 5699, 5749, 5799, 5849, 5899, 5949, 5999, 6049, 6099, 6149, 6199, 6249, 6299, 6349, 6399, 6449, 6499, 6549, 6599, 6649, 6699, 6749, 6799, 6849, 6899, 6949, 6999, 7049, 7099, 7149, 7199, 7249, 7299, 7349, 7399, 7449, 7499, 7549, 7599, 7649, 7699, 7749, 7799, 7849, 7899, 7949, 7999, 8049, 8099, 8149, 8199, 8249, 8299, 8349, 8399, 8449, 8499, 8510]
train_loss = [0.29812127351760864, 0.09726141393184662, 0.11050711572170258, 0.05295535549521446, 0.25072506070137024, 0.06347313523292542, 0.0074134827591478825, 0.007355031091719866, 0.005854050628840923, 0.1077655553817749, 0.03423212096095085, 0.004512341693043709, 0.05094479024410248, 0.029622329398989677, 0.009632650762796402, 0.07658009231090546, 0.0034999214112758636, 0.2270852029323578, 0.0062081413343548775, 0.17789892852306366, 0.004114548675715923, 0.14981728792190552, 0.036628659814596176, 0.0029653548263013363, 0.003450703574344516, 0.001960722729563713, 0.00656077079474926, 0.08923965692520142, 0.0016847928054630756, 0.0038995901122689247, 0.03633958846330643, 0.03195501118898392, 0.06696256250143051, 0.10591498017311096, 0.0013373694382607937, 0.03382914513349533, 0.041125115007162094, 0.004020444117486477, 0.007596381939947605, 0.0012353102210909128, 0.0011489391326904297, 0.1682731658220291, 0.002332075498998165, 0.0010709604248404503, 0.0015237173065543175, 0.01764373667538166, 0.01626446470618248, 0.005277498625218868, 0.030235521495342255, 0.003381753107532859, 0.07383914291858673, 0.0009331544861197472, 0.04183352366089821, 0.0012186686508357525, 0.10216294229030609, 0.0015323162078857422, 0.0010933877201750875, 0.0013343016617000103, 0.004044453147798777, 0.0690288096666336, 0.0009070078958757222, 0.0010812761029228568, 0.0058615608140826225, 0.02446659468114376, 0.11527096480131149, 0.0016414006240665913, 0.005098645109683275, 0.0013019004836678505, 0.0007382869371213019, 0.020343299955129623, 0.13775160908699036, 0.0017237584106624126, 0.08029236644506454, 0.00150864920578897, 0.210292249917984, 0.011901208199560642, 0.0006807963363826275, 0.14008904993534088, 0.00076096854172647, 0.05131581053137779, 0.011108470149338245, 0.034241802990436554, 0.07399069517850876, 0.002034521196037531, 0.04426271468400955, 0.0017738660098984838, 0.05166693031787872, 0.004173628985881805, 0.07203380018472672, 0.031100071966648102, 0.000715923379175365, 0.13104993104934692, 0.12962868809700012, 0.0006092389812693, 0.017575960606336594, 0.001288207364268601, 0.0006088097579777241, 0.04359978809952736, 0.1312134861946106, 0.06881897896528244, 0.045874349772930145, 0.060510169714689255, 0.0005124250892549753, 0.0006429037312045693, 0.009053925983607769, 0.0029715541750192642, 0.0006772836204618216, 0.13745228946208954, 0.0005376656772568822, 0.03993016853928566, 0.05084002763032913, 0.1411040872335434, 0.04816069081425667, 0.02256215550005436, 0.0005778948543593287, 0.0005297502502799034, 0.003073708154261112, 0.0599423348903656, 0.002019739244133234, 0.00045363110257312655, 0.06205907091498375, 0.0018437624676153064, 0.021445566788315773, 0.001769129536114633, 0.0004923662054352462, 0.054573021829128265, 0.04180542379617691, 0.07655976712703705, 0.003201794810593128, 0.0010943254455924034, 0.0008301257621496916, 0.038340263068675995, 0.0004049460112582892, 0.0011375268222764134, 0.0066940197721123695, 0.00043145817471668124, 0.000938844692427665, 0.00038992566987872124, 0.0004375299031380564, 0.08716510236263275, 0.6202362775802612, 0.0007092793239280581, 0.0015296221245080233, 0.00038135849172249436, 0.04211372137069702, 0.014710686169564724, 0.0009361903066746891, 0.0008485793950967491, 0.02848755568265915, 0.005245152860879898, 0.029052307829260826, 0.0007567247375845909, 0.00046226184349507093, 0.08077628165483475, 0.03795256465673447, 0.01873222552239895, 0.0009277979843318462, 0.00042530696373432875, 0.07229070365428925, 0.00043907167855650187, 0.06605907529592514, 0.062022432684898376, 0.0009582837228663266, 0.03534330427646637, 0.04981325939297676, 0.06928247958421707, 0.00042629241943359375, 0.0003938357112929225, 0.030700458213686943, 0.004894828889518976]
val_acc = [0.9851702451705933]
val_loss = [0.03726286068558693]

# Убедимся, что длины массивов совпадают
steps = steps[:len(train_loss)]  # Обрезаем steps до длины train_loss

# Построение графика
plt.figure(figsize=(12, 6))

# График train_loss
plt.plot(steps, train_loss, label='Train Loss', color='blue')

# График val_loss (если данные доступны)
if val_loss:
    plt.plot([steps[-1]], val_loss, 'o', label='Validation Loss', color='red')

# График val_acc (если данные доступны)
if val_acc:
    plt.plot([steps[-1]], val_acc, 'o', label='Validation Accuracy', color='green')

plt.xlabel('Steps')
plt.ylabel('Loss / Accuracy')
plt.title('Training Process')
plt.legend()
plt.grid(True)
plt.show()